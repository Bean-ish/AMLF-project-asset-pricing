{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cba0c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf18d3",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07be86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "005cc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c0cb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['r(t+1)'] = df.groupby('permno')['return'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "818362a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8e2c7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df.copy()\n",
    "for feature in ['mom1m', 'mom12m', 'chmom', 'mom36m', 'turn', 'dolvol', 'idiovol', 'beta', 'betasq', 'ep', 'sp', 'agr', 'nincr']:\n",
    "    df_filled[feature] = df_filled.groupby('Date')[feature].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "497d9787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             0\n",
       "permno           0\n",
       "return           0\n",
       "mom1m            0\n",
       "mom12m           0\n",
       "chmom            0\n",
       "indmom           0\n",
       "mom36m           0\n",
       "turn             0\n",
       "mvel1            0\n",
       "dolvol           0\n",
       "ill              0\n",
       "zerotrade        0\n",
       "baspread         0\n",
       "retvol           0\n",
       "idiovol          0\n",
       "beta             0\n",
       "betasq           0\n",
       "ep               0\n",
       "sp               0\n",
       "agr              0\n",
       "nincr            0\n",
       "return(t-1)    500\n",
       "r(t+1)         500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1e9f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['mom1m', 'mom12m', 'chmom', 'mom36m', 'turn', 'dolvol', 'idiovol', 'beta', 'betasq', 'ep', 'sp', 'agr', 'nincr']] = df_filled.loc[:,['mom1m', 'mom12m', 'chmom', 'mom36m', 'turn', 'dolvol', 'idiovol', 'beta', 'betasq', 'ep', 'sp', 'agr', 'nincr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "745aca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvel_sorted = df.sort_values('mvel1',ascending=False)\n",
    "top_100 = mvel_sorted.groupby('Date').head(100).reset_index(drop=True)\n",
    "bottom_100 = mvel_sorted.groupby('Date').tail(100).reset_index(drop=True)\n",
    "top_100.set_index('Date', inplace=True)\n",
    "bottom_100.set_index('Date', inplace=True)\n",
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ac410db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permno           0\n",
       "return           0\n",
       "mom1m            0\n",
       "mom12m           0\n",
       "chmom            0\n",
       "indmom           0\n",
       "mom36m           0\n",
       "turn             0\n",
       "mvel1            0\n",
       "dolvol           0\n",
       "ill              0\n",
       "zerotrade        0\n",
       "baspread         0\n",
       "retvol           0\n",
       "idiovol          0\n",
       "beta             0\n",
       "betasq           0\n",
       "ep               0\n",
       "sp               0\n",
       "agr              0\n",
       "nincr            0\n",
       "return(t-1)    500\n",
       "r(t+1)         500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840bae86",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a99fc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rolling_windows(data, total_years, initial_train_size, val_size):\n",
    "    total_years = total_years\n",
    "    windows = []\n",
    "    initial_train_size = initial_train_size\n",
    "    val_size = val_size\n",
    "\n",
    "    # Loop through the years to create rolling windows\n",
    "    for i in range(initial_train_size + 1, total_years-val_size):\n",
    "        idx_1 = '20' + str(i).zfill(2) + '-01-01'\n",
    "        idx_2 = '20' + str(i + val_size).zfill(2) + '-01-01'\n",
    "        \n",
    "        training = data[:idx_1].dropna()\n",
    "        validation = data[idx_1:idx_2]\n",
    "        testing = data[idx_2:].dropna()\n",
    "        \n",
    "        train_com = data[:idx_2].dropna()\n",
    "\n",
    "        windows.append((training, validation, testing, train_com))\n",
    "        \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c9e3107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>return</th>\n",
       "      <th>mom1m</th>\n",
       "      <th>mom12m</th>\n",
       "      <th>chmom</th>\n",
       "      <th>indmom</th>\n",
       "      <th>mom36m</th>\n",
       "      <th>turn</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>retvol</th>\n",
       "      <th>idiovol</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>ep</th>\n",
       "      <th>sp</th>\n",
       "      <th>agr</th>\n",
       "      <th>nincr</th>\n",
       "      <th>return(t-1)</th>\n",
       "      <th>r(t+1)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-02-28</th>\n",
       "      <td>13610</td>\n",
       "      <td>0.161318</td>\n",
       "      <td>-0.190508</td>\n",
       "      <td>0.327877</td>\n",
       "      <td>0.249243</td>\n",
       "      <td>0.018187</td>\n",
       "      <td>-0.285854</td>\n",
       "      <td>1.023598</td>\n",
       "      <td>8.012218e+05</td>\n",
       "      <td>13.974887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.058059</td>\n",
       "      <td>0.385289</td>\n",
       "      <td>0.148447</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>1.472909</td>\n",
       "      <td>0.325935</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.202242</td>\n",
       "      <td>-0.009186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-28</th>\n",
       "      <td>13856</td>\n",
       "      <td>0.046940</td>\n",
       "      <td>-0.110820</td>\n",
       "      <td>0.471192</td>\n",
       "      <td>-0.384316</td>\n",
       "      <td>0.115779</td>\n",
       "      <td>-0.027868</td>\n",
       "      <td>0.691164</td>\n",
       "      <td>6.359623e+07</td>\n",
       "      <td>17.810579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>0.043186</td>\n",
       "      <td>0.118710</td>\n",
       "      <td>0.014092</td>\n",
       "      <td>0.039970</td>\n",
       "      <td>0.397105</td>\n",
       "      <td>0.225463</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.112756</td>\n",
       "      <td>-0.041659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-28</th>\n",
       "      <td>13901</td>\n",
       "      <td>0.094426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.261056</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>0.682048</td>\n",
       "      <td>-0.446731</td>\n",
       "      <td>0.829092</td>\n",
       "      <td>9.783550e+07</td>\n",
       "      <td>17.863872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.050186</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.142695</td>\n",
       "      <td>1.148088</td>\n",
       "      <td>-0.024383</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.010566</td>\n",
       "      <td>0.002179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-28</th>\n",
       "      <td>13928</td>\n",
       "      <td>-0.084292</td>\n",
       "      <td>0.030857</td>\n",
       "      <td>0.431206</td>\n",
       "      <td>-0.111444</td>\n",
       "      <td>0.682048</td>\n",
       "      <td>-0.021776</td>\n",
       "      <td>0.842627</td>\n",
       "      <td>1.496689e+07</td>\n",
       "      <td>16.501171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.038102</td>\n",
       "      <td>0.142332</td>\n",
       "      <td>0.020258</td>\n",
       "      <td>0.051091</td>\n",
       "      <td>1.138525</td>\n",
       "      <td>-0.069288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033114</td>\n",
       "      <td>0.034813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-28</th>\n",
       "      <td>13936</td>\n",
       "      <td>0.149662</td>\n",
       "      <td>-0.006923</td>\n",
       "      <td>0.957347</td>\n",
       "      <td>0.098914</td>\n",
       "      <td>-0.032018</td>\n",
       "      <td>-0.421654</td>\n",
       "      <td>0.820512</td>\n",
       "      <td>3.525850e+05</td>\n",
       "      <td>12.565110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045878</td>\n",
       "      <td>0.068790</td>\n",
       "      <td>1.022686</td>\n",
       "      <td>1.045888</td>\n",
       "      <td>0.091598</td>\n",
       "      <td>6.902488</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>0.031713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-31</th>\n",
       "      <td>89279</td>\n",
       "      <td>0.048871</td>\n",
       "      <td>0.024940</td>\n",
       "      <td>-0.031547</td>\n",
       "      <td>0.072748</td>\n",
       "      <td>0.098944</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>0.065080</td>\n",
       "      <td>9.697706e+04</td>\n",
       "      <td>9.063220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.115212</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>0.060214</td>\n",
       "      <td>0.254025</td>\n",
       "      <td>-0.042325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026024</td>\n",
       "      <td>-0.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-31</th>\n",
       "      <td>89317</td>\n",
       "      <td>-0.054903</td>\n",
       "      <td>0.098958</td>\n",
       "      <td>-0.275862</td>\n",
       "      <td>0.327413</td>\n",
       "      <td>0.163590</td>\n",
       "      <td>0.304287</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>1.166978e+05</td>\n",
       "      <td>7.426549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016572</td>\n",
       "      <td>0.040691</td>\n",
       "      <td>0.271733</td>\n",
       "      <td>0.073839</td>\n",
       "      <td>-0.130443</td>\n",
       "      <td>0.836804</td>\n",
       "      <td>-0.183381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097319</td>\n",
       "      <td>-0.128977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-31</th>\n",
       "      <td>89456</td>\n",
       "      <td>0.160222</td>\n",
       "      <td>-0.045905</td>\n",
       "      <td>0.019759</td>\n",
       "      <td>0.342970</td>\n",
       "      <td>0.085635</td>\n",
       "      <td>0.545880</td>\n",
       "      <td>1.244986</td>\n",
       "      <td>3.647852e+05</td>\n",
       "      <td>13.235699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.054782</td>\n",
       "      <td>1.819325</td>\n",
       "      <td>3.309943</td>\n",
       "      <td>0.052840</td>\n",
       "      <td>1.019684</td>\n",
       "      <td>0.011990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.041839</td>\n",
       "      <td>-0.024223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-31</th>\n",
       "      <td>89790</td>\n",
       "      <td>-0.018619</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-0.213837</td>\n",
       "      <td>-0.241931</td>\n",
       "      <td>0.068689</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>5.602500e+04</td>\n",
       "      <td>10.551278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027389</td>\n",
       "      <td>0.110512</td>\n",
       "      <td>2.490410</td>\n",
       "      <td>6.202142</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.353447</td>\n",
       "      <td>-0.066737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.084549</td>\n",
       "      <td>-0.058698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-31</th>\n",
       "      <td>89915</td>\n",
       "      <td>-0.027124</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.035572</td>\n",
       "      <td>0.263321</td>\n",
       "      <td>0.122462</td>\n",
       "      <td>1.197060</td>\n",
       "      <td>0.672472</td>\n",
       "      <td>7.856415e+05</td>\n",
       "      <td>12.643065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026306</td>\n",
       "      <td>0.042038</td>\n",
       "      <td>1.130174</td>\n",
       "      <td>1.277294</td>\n",
       "      <td>0.059130</td>\n",
       "      <td>0.394275</td>\n",
       "      <td>-0.143708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161444</td>\n",
       "      <td>0.008394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35500 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            permno    return     mom1m    mom12m     chmom    indmom  \\\n",
       "Date                                                                   \n",
       "2001-02-28   13610  0.161318 -0.190508  0.327877  0.249243  0.018187   \n",
       "2001-02-28   13856  0.046940 -0.110820  0.471192 -0.384316  0.115779   \n",
       "2001-02-28   13901  0.094426  0.000000  1.261056  0.535897  0.682048   \n",
       "2001-02-28   13928 -0.084292  0.030857  0.431206 -0.111444  0.682048   \n",
       "2001-02-28   13936  0.149662 -0.006923  0.957347  0.098914 -0.032018   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "2006-12-31   89279  0.048871  0.024940 -0.031547  0.072748  0.098944   \n",
       "2006-12-31   89317 -0.054903  0.098958 -0.275862  0.327413  0.163590   \n",
       "2006-12-31   89456  0.160222 -0.045905  0.019759  0.342970  0.085635   \n",
       "2006-12-31   89790 -0.018619  0.080000 -0.213837 -0.241931  0.068689   \n",
       "2006-12-31   89915 -0.027124  0.166667 -0.035572  0.263321  0.122462   \n",
       "\n",
       "              mom36m      turn         mvel1     dolvol  ...    retvol  \\\n",
       "Date                                                     ...             \n",
       "2001-02-28 -0.285854  1.023598  8.012218e+05  13.974887  ...  0.028112   \n",
       "2001-02-28 -0.027868  0.691164  6.359623e+07  17.810579  ...  0.020727   \n",
       "2001-02-28 -0.446731  0.829092  9.783550e+07  17.863872  ...  0.032157   \n",
       "2001-02-28 -0.021776  0.842627  1.496689e+07  16.501171  ...  0.016428   \n",
       "2001-02-28 -0.421654  0.820512  3.525850e+05  12.565110  ...  0.045878   \n",
       "...              ...       ...           ...        ...  ...       ...   \n",
       "2006-12-31 -0.027805  0.065080  9.697706e+04   9.063220  ...  0.011695   \n",
       "2006-12-31  0.304287  0.025187  1.166978e+05   7.426549  ...  0.016572   \n",
       "2006-12-31  0.545880  1.244986  3.647852e+05  13.235699  ...  0.023065   \n",
       "2006-12-31  0.458716  0.814450  5.602500e+04  10.551278  ...  0.027389   \n",
       "2006-12-31  1.197060  0.672472  7.856415e+05  12.643065  ...  0.026306   \n",
       "\n",
       "             idiovol      beta    betasq        ep        sp       agr  nincr  \\\n",
       "Date                                                                            \n",
       "2001-02-28  0.058059  0.385289  0.148447  0.019041  1.472909  0.325935    5.0   \n",
       "2001-02-28  0.043186  0.118710  0.014092  0.039970  0.397105  0.225463    1.0   \n",
       "2001-02-28  0.050186  0.021928  0.000481  0.142695  1.148088 -0.024383    2.0   \n",
       "2001-02-28  0.038102  0.142332  0.020258  0.051091  1.138525 -0.069288    1.0   \n",
       "2001-02-28  0.068790  1.022686  1.045888  0.091598  6.902488  0.000838    1.0   \n",
       "...              ...       ...       ...       ...       ...       ...    ...   \n",
       "2006-12-31  0.018700  0.115212  0.013274  0.060214  0.254025 -0.042325    0.0   \n",
       "2006-12-31  0.040691  0.271733  0.073839 -0.130443  0.836804 -0.183381    0.0   \n",
       "2006-12-31  0.054782  1.819325  3.309943  0.052840  1.019684  0.011990    0.0   \n",
       "2006-12-31  0.110512  2.490410  6.202142  0.001406  0.353447 -0.066737    1.0   \n",
       "2006-12-31  0.042038  1.130174  1.277294  0.059130  0.394275 -0.143708    0.0   \n",
       "\n",
       "            return(t-1)    r(t+1)  \n",
       "Date                               \n",
       "2001-02-28    -0.202242 -0.009186  \n",
       "2001-02-28    -0.112756 -0.041659  \n",
       "2001-02-28     0.010566  0.002179  \n",
       "2001-02-28     0.033114  0.034813  \n",
       "2001-02-28     0.014335  0.031713  \n",
       "...                 ...       ...  \n",
       "2006-12-31     0.026024 -0.004154  \n",
       "2006-12-31     0.097319 -0.128977  \n",
       "2006-12-31    -0.041839 -0.024223  \n",
       "2006-12-31     0.084549 -0.058698  \n",
       "2006-12-31     0.161444  0.008394  \n",
       "\n",
       "[35500 rows x 23 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows = generate_rolling_windows(df, 19, 6, 4)\n",
    "\n",
    "# to get training sample of the first split:\n",
    "windows[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e4c8fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = windows[1][0].dropna()\n",
    "validation = windows[1][1].dropna()\n",
    "testing = windows[1][2].dropna()\n",
    "training_combined = windows[1][3].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "db135b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_train = training['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6ad3d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = validation.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_val = validation['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "02da496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_test = testing['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4283c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = training_combined.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_train_combined = training_combined['r(t+1)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a45c7",
   "metadata": {},
   "source": [
    "# Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b2b01c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters based on the paper\n",
    "depths = range(1, 7)  \n",
    "n_trees = 300         \n",
    "feature_splits = [3, 5, 10, 20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bee5744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_calc(actual, predicted):\n",
    "\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted).flatten()\n",
    "    \n",
    "    predicted = np.clip(predicted, 0, None)\n",
    "    \n",
    "    ss_res = np.sum((actual - predicted) ** 2)\n",
    "    \n",
    "    ss_tot = np.sum(actual ** 2)\n",
    "    \n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "beee4dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 0.015934424121926805\n",
      "1 5 0.016998555995249243\n",
      "1 10 0.017938935024490066\n",
      "1 20 0.017515150064221663\n",
      "2 3 0.019944332264875353\n",
      "2 5 0.021476930998315602\n",
      "2 10 0.023209275749752067\n",
      "2 20 0.022942203113467063\n",
      "3 3 0.022955123735833416\n",
      "3 5 0.024723381649466636\n",
      "3 10 0.02487746867723062\n",
      "3 20 0.02450776759632134\n",
      "4 3 0.025348727783720548\n",
      "4 5 0.023971992320902524\n",
      "4 10 0.025626734217333635\n",
      "4 20 0.02373654697005978\n",
      "5 3 0.025907638696189617\n",
      "5 5 0.02500752108948845\n",
      "5 10 0.024469495004495467\n",
      "5 20 0.022572568982237362\n",
      "6 3 0.027727769654823597\n",
      "6 5 0.025563536872103665\n",
      "6 10 0.023269100755110794\n",
      "6 20 0.020412093508111795\n",
      "Best hyperparameters (max_depth, max_features): (6, 3)\n",
      "Best R^2 OOS: 0.027727769654823597\n"
     ]
    }
   ],
   "source": [
    "roos_vals = {}\n",
    "\n",
    "for max_depth in depths:\n",
    "\n",
    "    for max_features in feature_splits:\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                                   max_depth = max_depth,\n",
    "                                   max_features = max_features,\n",
    "                                   random_state = 42)\n",
    "\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        predictions = rf.predict(X_val)\n",
    "\n",
    "        r_val = R_calc(y_val, predictions)\n",
    "\n",
    "        roos_vals[((max_depth, max_features))] = r_val\n",
    "        print(max_depth, max_features, r_val)\n",
    "\n",
    "best_hyperparameters = max(roos_vals, key=roos_vals.get)\n",
    "best_r = roos_vals[best_hyperparameters]\n",
    "\n",
    "print(\"Best hyperparameters (max_depth, max_features):\", best_hyperparameters)\n",
    "print(\"Best R^2 OOS:\", best_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eab4da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.022183029686349442\n",
      "1 0.020412093508111795\n",
      "2 0.03386507635323155\n",
      "3 0.014127609918471729\n",
      "4 0.008924418941873014\n",
      "5 0.0029172300575580623\n",
      "6 0.012955616291498884\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [153]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m y_train_combined \u001b[38;5;241m=\u001b[39m training_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr(t+1)\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators \u001b[38;5;241m=\u001b[39m n_trees,\n\u001b[1;32m     26\u001b[0m                            max_depth \u001b[38;5;241m=\u001b[39m depth,\n\u001b[1;32m     27\u001b[0m                            max_features \u001b[38;5;241m=\u001b[39m feature,\n\u001b[1;32m     28\u001b[0m                            random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m predictions \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     34\u001b[0m r_val \u001b[38;5;241m=\u001b[39m R_calc(y_val, predictions)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "roos_vals = {}\n",
    "depth = best_hyperparameters[0]\n",
    "feature = best_hyperparameters[1]\n",
    "\n",
    "for i in range(len(windows)):\n",
    "\n",
    "    training = windows[i][0].dropna()\n",
    "    validation = windows[i][1].dropna()\n",
    "    testing = windows[i][2].dropna()\n",
    "    training_combined = windows[i][3].dropna()\n",
    "    \n",
    "    X_train = training.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_train = training['r(t+1)']\n",
    "    \n",
    "    X_val = validation.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_val = validation['r(t+1)']\n",
    "    \n",
    "    X_test = testing.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_test = testing['r(t+1)']\n",
    "    \n",
    "    X_train_combined = training_combined.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_train_combined = training_combined['r(t+1)']\n",
    "\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                               max_depth = depth,\n",
    "                               max_features = feature,\n",
    "                               random_state = 42)\n",
    "\n",
    "    rf.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "    predictions = rf.predict(X_val)\n",
    "\n",
    "    r_val = R_calc(y_val, predictions)\n",
    "\n",
    "    roos_vals[(i)] = r_val\n",
    "    print(i, r_val)\n",
    "\n",
    "    #best_hyperparameters = max(roos_vals, key=roos_vals.get)\n",
    "    #best_r = roos_vals[best_hyperparameters]\n",
    "\n",
    "    #print(\"Best hyperparameters (max_depth, max_features):\", best_hyperparameters)\n",
    "    #print(\"Best R^2 OOS:\", best_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caede49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                                   max_depth = depth,\n",
    "                                   max_features = feature,\n",
    "                                   random_state = 42)\n",
    "        \n",
    "rf.fit(X_train_combined, y_train_combined)\n",
    "        \n",
    "predictions = rf.predict(X_val)\n",
    "        \n",
    "r_val = R_calc(y_val, predictions)\n",
    "        \n",
    "print(r_val)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rf.predict(X_test)\n",
    "val = R_calc(y_test, test_pred)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa9944",
   "metadata": {},
   "source": [
    "# Top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = generate_rolling_windows(top_100, 19, 6, 4)\n",
    "\n",
    "training = windows[1][0].dropna()\n",
    "validation = windows[1][1].dropna()\n",
    "testing = windows[1][2].dropna()\n",
    "training_combined = windows[1][3].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_train = training['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = validation.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_val = validation['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_test = testing['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = training_combined.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_train_combined = training_combined['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roos_vals = {}\n",
    "\n",
    "for max_depth in depths:\n",
    "\n",
    "    for max_features in feature_splits:\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                                   max_depth = max_depth,\n",
    "                                   max_features = max_features,\n",
    "                                   random_state = 42)\n",
    "\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        predictions = rf.predict(X_val)\n",
    "\n",
    "        r_val = R_calc(y_val, predictions)\n",
    "\n",
    "        roos_vals[((max_depth, max_features))] = r_val\n",
    "        print(max_depth, max_features, r_val)\n",
    "\n",
    "best_hyperparameters = max(roos_vals, key=roos_vals.get)\n",
    "best_r = roos_vals[best_hyperparameters]\n",
    "\n",
    "print(\"Best hyperparameters (max_depth, max_features):\", best_hyperparameters)\n",
    "print(\"Best R^2 OOS:\", best_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554eb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "roos_vals = {}\n",
    "depth = best_hyperparameters[0]\n",
    "feature = best_hyperparameters[1]\n",
    "\n",
    "for i in range(len(windows)):\n",
    "\n",
    "    training = windows[i][0].dropna()\n",
    "    validation = windows[i][1].dropna()\n",
    "    testing = windows[i][2].dropna()\n",
    "    training_combined = windows[i][3].dropna()\n",
    "    \n",
    "    X_train = training.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_train = training['r(t+1)']\n",
    "    \n",
    "    X_val = validation.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_val = validation['r(t+1)']\n",
    "    \n",
    "    X_test = testing.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_test = testing['r(t+1)']\n",
    "    \n",
    "    X_train_combined = training_combined.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_train_combined = training_combined['r(t+1)']\n",
    "\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                               max_depth = depth,\n",
    "                               max_features = feature,\n",
    "                               random_state = 42)\n",
    "\n",
    "    rf.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "    predictions = rf.predict(X_val)\n",
    "\n",
    "    r_val = R_calc(y_val, predictions)\n",
    "\n",
    "    roos_vals[(i)] = r_val\n",
    "    print(i, r_val)\n",
    "\n",
    "    #best_hyperparameters = max(roos_vals, key=roos_vals.get)\n",
    "    #best_r = roos_vals[best_hyperparameters]\n",
    "\n",
    "    #print(\"Best hyperparameters (max_depth, max_features):\", best_hyperparameters)\n",
    "    #print(\"Best R^2 OOS:\", best_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc020e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                                   max_depth = depth,\n",
    "                                   max_features = feature,\n",
    "                                   random_state = 42)\n",
    "        \n",
    "rf.fit(X_train_combined, y_train_combined)\n",
    "        \n",
    "predictions = rf.predict(X_val)\n",
    "        \n",
    "r_val = R_calc(y_val, predictions)\n",
    "        \n",
    "print(r_val)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rf.predict(X_test)\n",
    "val = R_calc(y_test, test_pred)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f511ca4",
   "metadata": {},
   "source": [
    "# Bottom 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5becb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = generate_rolling_windows(bottom_100, 19, 6, 4)\n",
    "\n",
    "training = windows[0][0].dropna()\n",
    "validation = windows[0][1].dropna()\n",
    "testing = windows[0][2].dropna()\n",
    "training_combined = windows[0][3].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_train = training['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa581686",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = validation.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_val = validation['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e458921",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_test = testing['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c41afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = training_combined.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "y_train_combined = training_combined['r(t+1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc490cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roos_vals = {}\n",
    "\n",
    "for max_depth in depths:\n",
    "\n",
    "    for max_features in feature_splits:\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                                   max_depth = max_depth,\n",
    "                                   max_features = max_features,\n",
    "                                   random_state = 42)\n",
    "\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        predictions = rf.predict(X_val)\n",
    "\n",
    "        r_val = R_calc(y_val, predictions)\n",
    "\n",
    "        roos_vals[((max_depth, max_features))] = r_val\n",
    "        print(max_depth, max_features, r_val)\n",
    "\n",
    "best_hyperparameters = max(roos_vals, key=roos_vals.get)\n",
    "best_r = roos_vals[best_hyperparameters]\n",
    "\n",
    "print(\"Best hyperparameters (max_depth, max_features):\", best_hyperparameters)\n",
    "print(\"Best R^2 OOS:\", best_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ab893",
   "metadata": {},
   "outputs": [],
   "source": [
    "roos_vals = {}\n",
    "depth = best_hyperparameters[0]\n",
    "feature = best_hyperparameters[1]\n",
    "\n",
    "for i in range(len(windows)):\n",
    "\n",
    "    training = windows[i][0].dropna()\n",
    "    validation = windows[i][1].dropna()\n",
    "    testing = windows[i][2].dropna()\n",
    "    training_combined = windows[i][3].dropna()\n",
    "    \n",
    "    X_train = training.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_train = training['r(t+1)']\n",
    "    \n",
    "    X_val = validation.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_val = validation['r(t+1)']\n",
    "    \n",
    "    X_test = testing.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_test = testing['r(t+1)']\n",
    "    \n",
    "    X_train_combined = training_combined.drop(columns = ['permno', 'return', 'r(t+1)'])\n",
    "    y_train_combined = training_combined['r(t+1)']\n",
    "\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                               max_depth = depth,\n",
    "                               max_features = feature,\n",
    "                               random_state = 42)\n",
    "\n",
    "    rf.fit(X_train_combined, y_train_co)\n",
    "\n",
    "    predictions = rf.predict(X_val)\n",
    "\n",
    "    r_val = R_calc(y_val, predictions)\n",
    "\n",
    "    roos_vals[(i)] = r_val\n",
    "    print(i, r_val)\n",
    "\n",
    "    #best_hyperparameters = max(roos_vals, key=roos_vals.get)\n",
    "    #best_r = roos_vals[best_hyperparameters]\n",
    "\n",
    "    #print(\"Best hyperparameters (max_depth, max_features):\", best_hyperparameters)\n",
    "    #print(\"Best R^2 OOS:\", best_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38286189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(n_estimators = n_trees,\n",
    "                                   max_depth = depth,\n",
    "                                   max_features = feature,\n",
    "                                   random_state = 42)\n",
    "        \n",
    "rf.fit(X_train_combined, y_train_combined)\n",
    "        \n",
    "predictions = rf.predict(X_val)\n",
    "        \n",
    "r_val = R_calc(y_val, predictions)\n",
    "        \n",
    "print(r_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rf.predict(X_test)\n",
    "val = R_calc(y_test, test_pred)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b333a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
